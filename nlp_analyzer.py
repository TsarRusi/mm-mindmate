import re
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
from textblob import TextBlob
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import logging

from config import settings

logger = logging.getLogger(__name__)

# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã NLTK (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('sentiment/vader_lexicon')
    nltk.data.find('corpora/stopwords')
except LookupError:
    import nltk.downloader
    nltk.download('punkt')
    nltk.download('vader_lexicon')
    nltk.download('stopwords')
    nltk.download('punkt_tab')

class NLPAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —ç–º–æ—Ü–∏–π, —Ç–µ–º –∏ —É—Ä–æ–≤–Ω—è —Å—Ç—Ä–µ—Å—Å–∞."""
    
    def __init__(self, language: str = "russian"):
        self.language = language
        self.sia = SentimentIntensityAnalyzer()
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        self.stop_words = set(stopwords.words('russian' if language == 'ru' else 'english'))
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º
        self.topic_keywords = {
            '—Ä–∞–±–æ—Ç–∞': ['—Ä–∞–±–æ—Ç–∞', '–Ω–∞—á–∞–ª—å–Ω–∏–∫', '–∫–æ–ª–ª–µ–≥–∞', '–¥–µ–¥–ª–∞–π–Ω', '–ø—Ä–æ–µ–∫—Ç', '–æ—Ñ–∏—Å', '–∑–∞—Ä–ø–ª–∞—Ç–∞', 
                      '—Å–æ–≤–µ—â–∞–Ω–∏–µ', '–∑–∞–¥–∞—á–∞', '—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ', '–∫–∞—Ä—å–µ—Ä–∞'],
            '—Å–µ–º—å—è': ['—Å–µ–º—å—è', '—Ä–æ–¥–∏—Ç–µ–ª–∏', '–¥–µ—Ç–∏', '–º—É–∂', '–∂–µ–Ω–∞', '–±—Ä–∞—Ç', '—Å–µ—Å—Ç—Ä–∞', '—Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–∏',
                     '–æ—Ç–Ω–æ—à–µ–Ω–∏—è', '—Ä–∞–∑–≤–æ–¥', '–±—Ä–∞–∫', '—Å–µ–º–µ–π–Ω—ã–π'],
            '–∑–¥–æ—Ä–æ–≤—å–µ': ['–∑–¥–æ—Ä–æ–≤—å–µ', '–±–æ–ª–µ–∑–Ω—å', '–±–æ–ª—å', '–≤—Ä–∞—á', '–±–æ–ª—å–Ω–∏—Ü–∞', '–ª–µ–∫–∞—Ä—Å—Ç–≤–æ', '—Å–∏–º–ø—Ç–æ–º',
                        '—É—Å—Ç–∞–ª–æ—Å—Ç—å', '—Å–æ–Ω', '–±–µ—Å—Å–æ–Ω–Ω–∏—Ü–∞', '–¥–∏–µ—Ç–∞', '—Å–ø–æ—Ä—Ç'],
            '—Ñ–∏–Ω–∞–Ω—Å—ã': ['–¥–µ–Ω—å–≥–∏', '—Ñ–∏–Ω–∞–Ω—Å—ã', '–¥–æ–ª–≥', '–∫—Ä–µ–¥–∏—Ç', '–∑–∞—Ä–ø–ª–∞—Ç–∞', '—ç–∫–æ–Ω–æ–º–∏—è', '—Ç—Ä–∞—Ç—ã',
                       '–±—é–¥–∂–µ—Ç', '–Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '–±–µ–¥–Ω–æ—Å—Ç—å'],
            '—É—á–µ–±–∞': ['—É—á–µ–±–∞', '—ç–∫–∑–∞–º–µ–Ω', '—Å–µ—Å—Å–∏—è', '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å', '—Å—Ç—É–¥–µ–Ω—Ç', '–∑–∞—á–µ—Ç', '–∫—É—Ä—Å–æ–≤–∞—è',
                     '–¥–∏–ø–ª–æ–º', '–ª–µ–∫—Ü–∏—è', '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç'],
            '–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ': ['–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ', '–æ–¥–∏–Ω–æ–∫–∏–π', '–ø–æ–∫–∏–Ω—É—Ç—ã–π', '–∏–∑–æ–ª—è—Ü–∏—è', '–æ—Ç–≤–µ—Ä–≥–Ω—É—Ç—ã–π',
                           '–ø–æ–∫–∏–¥–∞—Ç—å', '–±—Ä–æ—Å–∏—Ç—å', '–ø–æ–∫–∏–¥–∞—Ç—å', '–Ω–µ–ª—é–±–∏–º—ã–π'],
            '—Ç—Ä–µ–≤–æ–≥–∞': ['—Ç—Ä–µ–≤–æ–≥–∞', '–ø–∞–Ω–∏–∫–∞', '—Å—Ç—Ä–∞—Ö', '–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ', '–Ω–µ—Ä–≤—ã', '—Å—Ç—Ä–µ—Å—Å', '–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ',
                       '–≤–æ–ª–Ω–µ–Ω–∏–µ', '–∏—Å–ø—É–≥', '—Ñ–æ–±–∏—è'],
            '–¥–µ–ø—Ä–µ—Å—Å–∏—è': ['–¥–µ–ø—Ä–µ—Å—Å–∏—è', '–∞–ø–∞—Ç–∏—è', '—Ç–æ—Å–∫–∞', '–≥—Ä—É—Å—Ç—å', '–±–µ–∑–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å', '–æ—Ç—á–∞—è–Ω–∏–µ',
                         '–ø–µ—á–∞–ª—å', '–º–µ–ª–∞–Ω—Ö–æ–ª–∏—è', '–ø–æ–¥–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å', '—Å—É–∏—Ü–∏–¥'],
        }
        
        # –ö—Ä–∏–∑–∏—Å–Ω—ã–µ —Å–ª–æ–≤–∞ (—Ç—Ä–∏–≥–≥–µ—Ä—ã –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∫—Ä–∏–∑–∏—Å–Ω–æ–≥–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞)
        self.crisis_keywords = [
            '—Å—É–∏—Ü–∏–¥', '—Å–∞–º–æ—É–±–∏–π—Å—Ç–≤–æ', '–ø–æ–∫–æ–Ω—á–∏—Ç—å', '—Å–≤–µ—Å—Ç–∏ —Å—á–µ—Ç—ã', '–Ω–µ —Ö–æ—á—É –∂–∏—Ç—å',
            '–≤—Å–µ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ', '–∫–æ–Ω–µ—Ü', '–Ω–∞–¥–æ–µ–ª–æ –∂–∏—Ç—å', '—É—Å—Ç–∞–ª –æ—Ç –∂–∏–∑–Ω–∏',
            '–ª—É—á—à–µ —É–º–µ—Ä–µ—Ç—å', '–Ω–µ –≤–∏–∂—É —Å–º—ã—Å–ª–∞', '–≤—Å–µ –ø–ª–æ—Ö–æ', '–Ω–µ—Ç –≤—ã—Ö–æ–¥–∞'
        ]
        
        logger.info(f"NLP –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —è–∑—ã–∫–∞: {language}")
    
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        if not text or len(text.strip()) < 3:
            return self._empty_result()
        
        try:
            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_text = self._clean_text(text)
            
            # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
            sentiment = self._analyze_sentiment(cleaned_text)
            topics = self._extract_topics(cleaned_text)
            stress_level = self._calculate_stress_level(cleaned_text, sentiment)
            emotions = self._detect_emotions(cleaned_text)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫—Ä–∏–∑–∏—Å–Ω—ã–µ —Å–ª–æ–≤–∞
            is_crisis, crisis_words = self._check_crisis_keywords(cleaned_text)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            word_count = len(cleaned_text.split())
            readability = self._calculate_readability(cleaned_text)
            
            result = {
                'text_original': text,
                'text_cleaned': cleaned_text,
                'sentiment': sentiment,
                'topics': topics,
                'stress_level': stress_level,
                'emotions': emotions,
                'is_crisis': is_crisis,
                'crisis_words_found': crisis_words,
                'metrics': {
                    'word_count': word_count,
                    'readability_score': readability,
                    'timestamp': datetime.utcnow().isoformat()
                },
                'recommendations': self._generate_recommendations(
                    sentiment, topics, stress_level, is_crisis
                )
            }
            
            logger.debug(f"NLP –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {result['sentiment']['label']}, —Å—Ç—Ä–µ—Å—Å: {stress_level}")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—Å—Ç–∞: {e}", exc_info=True)
            return self._error_result(str(e))
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞."""
        # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏, email, —Ö—ç—à—Ç–µ–≥–∏
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        text = re.sub(r'\S*@\S*\s?', '', text)
        text = re.sub(r'#\S+', '', text)
        
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'[^\w\s.,!?;:()-]', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        return text.lower()
    
    def _analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞."""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º TextBlob –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ/–∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
            if self.language == 'ru':
                # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º VADER (—Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ —á–µ–º TextBlob)
                scores = self.sia.polarity_scores(text)
                compound = scores['compound']
                
                if compound >= 0.05:
                    label = "POSITIVE"
                elif compound <= -0.05:
                    label = "NEGATIVE"
                else:
                    label = "NEUTRAL"
                    
                return {
                    'label': label,
                    'compound': compound,
                    'positive': scores['pos'],
                    'neutral': scores['neu'],
                    'negative': scores['neg']
                }
            else:
                # –î–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ TextBlob
                blob = TextBlob(text)
                polarity = blob.sentiment.polarity
                subjectivity = blob.sentiment.subjectivity
                
                if polarity > 0:
                    label = "POSITIVE"
                elif polarity < 0:
                    label = "NEGATIVE"
                else:
                    label = "NEUTRAL"
                    
                return {
                    'label': label,
                    'polarity': polarity,
                    'subjectivity': subjectivity
                }
                
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
            return {'label': 'NEUTRAL', 'error': str(e)}
    
    def _extract_topics(self, text: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        topics = []
        tokens = word_tokenize(text, language='russian' if self.language == 'ru' else 'english')
        
        for topic_name, keywords in self.topic_keywords.items():
            matches = []
            confidence = 0
            
            for keyword in keywords:
                if keyword in text:
                    matches.append(keyword)
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –∫–∞–∂–¥–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    confidence += 0.3
            
            if matches:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                confidence = min(1.0, confidence)
                topics.append({
                    'name': topic_name,
                    'keywords_found': matches,
                    'confidence': round(confidence, 2)
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        topics.sort(key=lambda x: x['confidence'], reverse=True)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º
        return topics[:5]
    
    def _calculate_stress_level(self, text: str, sentiment: Dict[str, Any]) -> int:
        """–†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è —Å—Ç—Ä–µ—Å—Å–∞ (1-10)."""
        stress_score = 5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
        
        # 1. –í–ª–∏—è–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        if sentiment.get('label') == 'NEGATIVE':
            compound = abs(sentiment.get('compound', 0) or sentiment.get('polarity', 0))
            if compound > 0.3:
                stress_score += 3
            elif compound > 0.1:
                stress_score += 2
            else:
                stress_score += 1
        
        # 2. –í–ª–∏—è–Ω–∏–µ —Ç–µ–º
        topics = self._extract_topics(text)
        stress_topics = {'—Ç—Ä–µ–≤–æ–≥–∞', '–¥–µ–ø—Ä–µ—Å—Å–∏—è', '–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ', '—Ñ–∏–Ω–∞–Ω—Å—ã'}
        
        for topic in topics:
            if topic['name'] in stress_topics:
                stress_score += 1
        
        # 3. –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤ —Ç—Ä–µ–≤–æ–≥–∏
        anxiety_words = ['—Ç—Ä–µ–≤–æ–∂', '–ø–∞–Ω–∏–∫', '—Å—Ç—Ä–∞—Ö', '–±–æ—é—Å—å', '–Ω–µ—Ä–≤', '—Å—Ç—Ä–µ—Å—Å']
        anxiety_count = sum(1 for word in anxiety_words if word in text)
        stress_score += min(anxiety_count, 2)
        
        # 4. –î–ª–∏–Ω–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–µ–∫—Å—Ç–∞
        words = text.split()
        if len(words) < 10:  # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è —á–∞—Å—Ç–æ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –ø–æ–¥–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å
            stress_score += 1
        elif len(words) > 100:  # –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ - –Ω–∞ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–µ –æ–±–¥—É–º—ã–≤–∞–Ω–∏–µ
            stress_score += 1
        
        # 5. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
        if '!' in text and text.count('!') > 3:
            stress_score += 1
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω 1-10
        return max(1, min(10, stress_score))
    
    def _detect_emotions(self, text: str) -> List[Dict[str, Any]]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π –≤ —Ç–µ–∫—Å—Ç–µ."""
        emotions = []
        
        # –°–ª–æ–≤–∞—Ä—å —ç–º–æ—Ü–∏–π –∏ –∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        emotion_dict = {
            '—Ä–∞–¥–æ—Å—Ç—å': ['—Ä–∞–¥', '—Å—á–∞—Å—Ç–ª–∏–≤', '—É—Ä–∞', '–æ—Ç–ª–∏—á–Ω–æ', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '–≤–æ—Å—Ç–æ—Ä–≥'],
            '–≥—Ä—É—Å—Ç—å': ['–≥—Ä—É—Å—Ç–Ω–æ', '–ø–µ—á–∞–ª—å–Ω–æ', '—Ç–æ—Å–∫–ª–∏–≤–æ', '–ø–ª–∞–∫–∞—Ç—å', '—Å–ª–µ–∑—ã', '—É–Ω—ã–Ω–∏–µ'],
            '–≥–Ω–µ–≤': ['–∑–ª–æ–π', '—Å–µ—Ä–¥–∏—Ç', '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω', '–±–µ—Å–∏—Ç', '–Ω–µ–Ω–∞–≤–∏–∂—É', '—è—Ä–æ—Å—Ç—å', '–≤–æ–∑–º—É—â–µ–Ω'],
            '—Å—Ç—Ä–∞—Ö': ['–±–æ—é—Å—å', '—Å—Ç—Ä–∞—à–Ω–æ', '–∏—Å–ø—É–≥', '—É–∂–∞—Å', '–ø–∞–Ω–∏–∫–∞', '—Ç—Ä–µ–≤–æ–≥–∞'],
            '—É–¥–∏–≤–ª–µ–Ω–∏–µ': ['—É–¥–∏–≤–ª–µ–Ω', '–Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ', '–æ–≥–æ', '–≤–∞—É', '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ', '–ø–æ—Ç—Ä—è—Å–∞—é—â–µ'],
            '—Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ': ['—Å–ø–æ–∫–æ–µ–Ω', '—É–º–∏—Ä–æ—Ç–≤–æ—Ä–µ–Ω', '—Ç–∏—à–∏–Ω–∞', '–º–∏—Ä', '—Ä–∞—Å—Å–ª–∞–±–ª–µ–Ω', '–≥–∞—Ä–º–æ–Ω–∏—è'],
        }
        
        for emotion, keywords in emotion_dict.items():
            matches = []
            for keyword in keywords:
                if keyword in text:
                    matches.append(keyword)
            
            if matches:
                confidence = min(1.0, len(matches) * 0.2)
                emotions.append({
                    'name': emotion,
                    'keywords_found': matches,
                    'confidence': round(confidence, 2)
                })
        
        return emotions
    
    def _check_crisis_keywords(self, text: str) -> Tuple[bool, List[str]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫—Ä–∏–∑–∏—Å–Ω—ã—Ö —Å–ª–æ–≤."""
        found_words = []
        for keyword in self.crisis_keywords:
            if keyword in text:
                found_words.append(keyword)
        
        return len(found_words) > 0, found_words
    
    def _calculate_readability(self, text: str) -> float:
        """–†–∞—Å—á–µ—Ç —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞."""
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return 0
        
        words = text.split()
        if not words:
            return 0
        
        avg_sentence_length = len(words) / len(sentences)
        avg_word_length = sum(len(word) for word in words) / len(words)
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏
        readability = 100 - (avg_sentence_length * 1.5 + avg_word_length * 10)
        return max(0, min(100, readability))
    
    def _generate_recommendations(self, sentiment: Dict[str, Any], 
                                 topics: List[Dict[str, Any]], 
                                 stress_level: int, 
                                 is_crisis: bool) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞."""
        recommendations = []
        
        if is_crisis:
            recommendations.append("‚ö†Ô∏è **–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç—Ä–µ–≤–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞.** –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∑–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –ø–æ–º–æ—â—å—é.")
        
        if stress_level >= 8:
            recommendations.append("üßò **–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–µ—Å—Å–∞.** –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ç–µ—Ö–Ω–∏–∫—É –¥—ã—Ö–∞–Ω–∏—è 4-7-8.")
            
        if stress_level >= 6:
            recommendations.append("üìù **–ó–∞–ø–∏—Å—ã–≤–∞–π—Ç–µ –º—ã—Å–ª–∏.** –í–µ–¥–µ–Ω–∏–µ –¥–Ω–µ–≤–Ω–∏–∫–∞ –ø–æ–º–æ–≥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏—è.")
        
        if sentiment.get('label') == 'NEGATIVE':
            if any(topic['name'] in ['—Ä–∞–±–æ—Ç–∞', '—Ñ–∏–Ω–∞–Ω—Å—ã'] for topic in topics):
                recommendations.append("üíº **–ü—Ä–æ–±–ª–µ–º—ã –Ω–∞ —Ä–∞–±–æ—Ç–µ/—Å —Ñ–∏–Ω–∞–Ω—Å–∞–º–∏.** –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ç–µ—Ö–Ω–∏–∫—É '—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞ —á–∞—Å—Ç–∏'.")
        
        if any(topic['name'] == '–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ' for topic in topics):
            recommendations.append("üë• **–ß—É–≤—Å—Ç–≤–æ –æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–∞.** –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–∏—Ç—å—Å—è –∫ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≥—Ä—É–ø–ø–∞–º –ø–æ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º.")
        
        if not recommendations:
            recommendations.append("üëç **–ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Å–∞–º–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ.** –†–µ–≥—É–ª—è—Ä–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –≤–µ–¥–µ—Ç –∫ –ª—É—á—à–µ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–µ–±—è.")
        
        return recommendations
    
    def _empty_result(self) -> Dict[str, Any]:
        """–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ –ø—É—Å—Ç–æ–º —Ç–µ–∫—Å—Ç–µ."""
        return {
            'text_original': '',
            'text_cleaned': '',
            'sentiment': {'label': 'NEUTRAL', 'compound': 0},
            'topics': [],
            'stress_level': 5,
            'emotions': [],
            'is_crisis': False,
            'crisis_words_found': [],
            'metrics': {
                'word_count': 0,
                'readability_score': 0,
                'timestamp': datetime.utcnow().isoformat()
            },
            'recommendations': ['–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞']
        }
    
    def _error_result(self, error_msg: str) -> Dict[str, Any]:
        """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ –∞–Ω–∞–ª–∏–∑–∞."""
        return {
            'text_original': '',
            'text_cleaned': '',
            'sentiment': {'label': 'ERROR', 'error': error_msg},
            'topics': [],
            'stress_level': 5,
            'emotions': [],
            'is_crisis': False,
            'crisis_words_found': [],
            'metrics': {
                'word_count': 0,
                'readability_score': 0,
                'timestamp': datetime.utcnow().isoformat()
            },
            'recommendations': ['–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ']
        }
    
    def get_text_summary(self, analysis_result: Dict[str, Any]) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –∞–Ω–∞–ª–∏–∑–∞."""
        if not analysis_result or 'sentiment' not in analysis_result:
            return "–ê–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
        
        sentiment = analysis_result['sentiment']
        stress = analysis_result.get('stress_level', 5)
        topics = analysis_result.get('topics', [])
        
        summary_parts = []
        
        # –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        if sentiment.get('label') == 'POSITIVE':
            summary_parts.append("üìà **–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π –Ω–∞—Å—Ç—Ä–æ–π**")
        elif sentiment.get('label') == 'NEGATIVE':
            summary_parts.append("üìâ **–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –Ω–∞—Å—Ç—Ä–æ–π**")
        else:
            summary_parts.append("üìä **–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –Ω–∞—Å—Ç—Ä–æ–π**")
        
        # –£—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–µ—Å—Å–∞
        if stress >= 8:
            summary_parts.append(f"üî¥ **–í—ã—Å–æ–∫–∏–π —Å—Ç—Ä–µ—Å—Å:** {stress}/10")
        elif stress >= 6:
            summary_parts.append(f"üü° **–ü–æ–≤—ã—à–µ–Ω–Ω—ã–π —Å—Ç—Ä–µ—Å—Å:** {stress}/10")
        elif stress <= 4:
            summary_parts.append(f"üü¢ **–ù–∏–∑–∫–∏–π —Å—Ç—Ä–µ—Å—Å:** {stress}/10")
        else:
            summary_parts.append(f"‚ö™ **–°—Ä–µ–¥–Ω–∏–π —Å—Ç—Ä–µ—Å—Å:** {stress}/10")
        
        # –¢–µ–º—ã
        if topics:
            main_topics = [t['name'] for t in topics[:3]]
            summary_parts.append(f"üè∑Ô∏è **–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã:** {', '.join(main_topics)}")
        
        # –ö—Ä–∏–∑–∏—Å–Ω—ã–π —Ñ–ª–∞–≥
        if analysis_result.get('is_crisis', False):
            summary_parts.append("üö® **–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç—Ä–µ–≤–æ–∂–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã**")
        
        return "\n".join(summary_parts)

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
nlp_analyzer = NLPAnalyzer(language=settings.LANGUAGE)